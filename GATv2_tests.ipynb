{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATv2 TEST \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2708\n",
    "in_features = 1433\n",
    "out_features = 64\n",
    "alpha = 0.2\n",
    "adj = torch.tensor(np.ones((N,N)), dtype=torch.float)\n",
    "h = torch.tensor(np.random.randn(N,in_features), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W = nn.Parameter(torch.empty(size=(in_features, 2*out_features)))\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "a = nn.Parameter(torch.empty(size=(out_features, 1)))\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "\n",
    "#Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "Wh1 = torch.matmul(h,W[:, :out_features]) #N, F @ F , F' --> N, F'\n",
    "Wh2 = torch.matmul(h,W[:, out_features:])\n",
    "e_0 = Wh1 + Wh2 \n",
    "e_1 = leakyrelu(e_0)\n",
    "e = torch.matmul(e_1, a) #N=2708, F'=64 @ F' --> N\n",
    "      \n",
    "#e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "zero_vec = -9e15*torch.ones_like(e)\n",
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime = torch.matmul(attention, Wh2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OTHER TEST \n",
    "W = nn.Parameter(torch.empty(size=(2*in_features, out_features)))\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "a = nn.Parameter(torch.empty(size=(out_features, 1)))\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "\n",
    "#Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "Wh1 = torch.matmul(h,W[:in_features, :]) #N, F @ F , F' --> N, F'\n",
    "Wh2 = torch.matmul(h,W[in_features:, :])\n",
    "e_0 = Wh1 + Wh2 \n",
    "e_1 = leakyrelu(e_0)\n",
    "e = torch.matmul(e_1, a) #N=2708, F'=64 @ F' --> N\n",
    "      \n",
    "#e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "zero_vec = -9e15*torch.ones_like(e)\n",
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime = torch.matmul(attention, Wh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 64])\n",
      "torch.Size([2708, 64])\n",
      "torch.Size([2708, 64])\n",
      "torch.Size([2708, 64])\n",
      "torch.Size([64, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Wh1.shape)\n",
    "print(Wh2.shape)\n",
    "print(e_0.shape)\n",
    "print(e_1.shape)\n",
    "print(a.shape)\n",
    "h_prime.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
